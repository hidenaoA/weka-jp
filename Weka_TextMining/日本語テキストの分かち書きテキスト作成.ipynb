{"cells":[{"cell_type":"markdown","metadata":{"id":"PwDDonzwTYno"},"source":["#日本語分かち書きテキストの作成\n","\n","\n","\n","1.   収集したテキストを1行1文書（段落など）のテキストファイルとして作成する．このとき，テキストファイルはUTF-8(N)で保存する\n","2.   1.で作成したテキストファイルを（通常は左側にある）ファイルのアイコンを開いた領域（プログラムからは/content/に見える）にアップロードする\n","3. 2.でアップロードしたファイル名と分かち書き後のファイルを下記のコードセルに書き込む\n","4. 下記のコードセルを実行する（↑のメニューにある”ランタイム”から”すべてのセル実行”で実行）\n","5. 生成されたテキストファイル(*_wakati.txt)をダウンロードする\n","6. クラスタリングの場合は，そのまま利用する．分類の場合は，分類ラベルの列とテキストの列からなるCSVファイルを作成し，ARFFに編集する\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fF3WC86u5iwf"},"outputs":[],"source":["#Mecabの基本パッケージと開発用パッケージをインストールしておく\n","!apt -q -y install mecab libmecab-dev file\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9ENE1h8VW0uS"},"outputs":[],"source":["# Pythonで品詞を選択しつつ分かち書き（MeCab + UniDic + Neologd）\n","!pip install mecab-python3\n","!pip install neologdn\n","!pip install unidic\n","!python -m unidic download\n","\n","# 下記のコマンドを実行（先頭の#を消去して実行）するとNeologdの辞書がインストールされる（かなり時間がかかります）\n","#!git clone --depth 1 https://github.com/neologd/mecab-unidic-neologd.git\n","#!echo yes | mecab-unidic-neologd/bin/install-mecab-unidic-neologd -n\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oO8MI7st3rWk"},"outputs":[],"source":["# テキストファイルを開いて，一行ずつ分かち書きを行う\n","import MeCab\n","import unidic\n","import neologdn\n","import re\n","\n","txt_file_name = 'test.txt' #アップロードするテキストファイル名にする←※要書き換え\n","result_file_name = 'test_wakati.txt' #出力する分かち書きされたファイル名←※要書き換え\n","with open(result_file_name,'w') as f:\n","  f.write('')\n","target_pos_tags = ['名詞', '動詞', '形容詞', '形容動詞', '副詞'] #分かち書き後に対象とする品詞\n","stop_words = ['の','さ'] #間違って形態素解析解析をされるなどして，不要になる単語\n","\n","tagger = MeCab.Tagger() #UniDicのまま使うのであれば，こちらをつかう\n","#tagger = MeCab.Tagger('-d /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-unidic-neologd') #Neologdを使うのであれば，こちらを使う\n","\n","#テキストファイルを開いて各行（1文書＝1段落や1見出しなど）\n","with open(txt_file_name, 'r') as f:\n","  line = f.readline()\n","  while line:\n","    result_line_array = []\n","    #各行の内容を正規化（内容をきれいにする）\n","    line = neologdn.normalize(line)\n","    #HTMLタグを取り除く\n","    line = re.sub(r\"<(.+?)>\",\"\",line)\n","    #URL（のパターン）を取り除く\n","    line = re.sub(r\"(https?|ftp)(:\\/\\/[-_\\.!~*\\'()a-zA-Z0-9;\\/?:\\@&=\\+\\$,%#]+)\", \"\" ,line)\n","    parsed_lines = tagger.parse(line).split('\\n')\n","    #print(parsed_lines)\n","    for token in parsed_lines: #各行を形態素解析\n","      m_result = str(token).split('\\t') #形態素結果を表層形とその他の結果に分割\n","      face = m_result[0] #表層形を取り出す\n","      if face == \"EOS\": break\n","      face = face.replace(',','，')\n","      word = face #単語を表層形の単語とする\n","      #word = m_info_list[6] #ここでfaceの代わりにm_info_list[6]を集計対象とすると”原形”で数え上げが可能になる\n","      #print(str(m_result[0])+\" \"+m_result[1]) #←形態素解析結果を出力する場合は先頭のコメントを外す\n","      m_info_list = m_result[1].split(',') #形態素解析結果（品詞以降）を分割\n","      m_pos = m_info_list[0] #形態素解析結果の先頭の品詞（大分類）を取り出す(1:中分類、2:小分類)\n","      #print(face+' '+str(m_info_list))\n","      if m_pos in target_pos_tags and word not in stop_words: #対象の品詞のリスト内に解析結果の品詞があれば分かち書き対象の単語とする\n","        result_line_array.append(word)\n","\n","    #分かち書き結果をファイルに出力する\n","    with open(result_file_name,'a') as rf:\n","      rf.write('\"'+' '.join(result_line_array)+'\"\\n') #各行を\"で挟むのを忘れずに！\n","\n","    #テキストファイルの次の行を読み込む\n","    line = f.readline()\n"]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP49y/SiKF+yYcyjlYlsBn/"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}